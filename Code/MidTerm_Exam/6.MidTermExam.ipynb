{"cells":[{"cell_type":"markdown","metadata":{"id":"8JqcPJnLqdN8"},"source":["# 1. Lý Thuyết"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# 1. C\n","# 2. B\n","# 3. C\n","# 4. B\n","# 5. B\n","# 6. B\n","# 7. C\n","# 8. B\n","# 9. B\n","# 10. C \n","# 11. C \n","# 12. B \n","# 13. C \n","# 14. B \n","# 15. B \n","# 16. B \n","# 17. C \n","# 18. C \n","# 19. C \n","# 20. C"]},{"cell_type":"markdown","metadata":{"id":"Q1ncZcvQrLTn"},"source":["1) What is the limitations of LLMs in terms of factual accuracy and reasoning abilities?\n","\n","A) LLMs are always accurate and consistent in their outputs.\n","\n","B) LLMs excel at complex reasoning tasks, such as logical inference and multi-step problem solving.\n","\n","C) LLMs may produce factually incorrect or inconsistent information, struggle with complex reasoning tasks, and exhibit biases from their training data.\n","\n","D) LLMs can independently acquire domain-specific knowledge without fine-tuning or additional data."]},{"cell_type":"markdown","metadata":{"id":"qETuYZNrrWqa"},"source":["2) What are some ethical considerations surrounding the use of LLMs?\n","\n","A) Ensuring LLMs can generate multilingual content and improving their processing speed.\n","\n","B) Addressing concerns related to privacy and data protection, bias and discrimination, intellectual property, misuse and malicious applications, and environmental impact.\n","\n","C) Developing LLMs with a focus on increasing their size and computational power.\n","\n","D) Ensuring LLMs are only used for entertainment purposes to avoid ethical concerns."]},{"cell_type":"markdown","metadata":{"id":"56TJ3qkzr3pI"},"source":["3) Explain the concept of few-shot learning and its applications in fine-tuning LLMs.\n","\n","A) Few-shot learning is a technique where the model is trained with a large number of labeled instances to ensure accuracy.\n","\n","B) Few-shot learning involves training LLMs with no labeled instances and relies on unsupervised learning.\n","\n","C) Few-shot learning is a fine-tuning strategy where the model is given a limited number of instances (usually 1 to 5) to tailor it to a specific task or domain, enabling it to learn and generalize from a few examples.\n","\n","D) Few-shot learning is used to optimize LLMs for general tasks without the need for domain-specific data."]},{"cell_type":"markdown","metadata":{"id":"Ycb8AHVAsHXG"},"source":["4) Explain the concept of self-attention and its role in LLM performance.\n","\n","A) Self-attention involves giving equal weights to all parts of the input sequence to ensure uniform processing.\n","\n","B) Self-attention assigns various weights to different parts of the input sequence, helping the model capture contextual information and long-range relationships, enhancing performance in tasks like content production, machine translation, and language understanding.\n","\n","C) Self-attention focuses on short-term dependencies in the input sequence, improving performance in sequential data processing.\n","\n","D) Self-attention ignores the positional information of words, leading to better handling of unordered data."]},{"cell_type":"markdown","metadata":{"id":"6Vzyush9s7Cg"},"source":["5) Explore the potential future applications of LLMs in various industries.\n","\n","A) LLMs will mainly be used for hardware design and engineering simulations.\n","\n","B) LLMs can transform industries by aiding in medical communication and diagnosis, legal research, educational content creation, creative content generation, and customer service through chatbots and virtual assistants.\n","\n","C) LLMs are expected to replace human workers entirely in every industry.\n","\n","D) The primary application of LLMs will be in agricultural automation and livestock management."]},{"cell_type":"markdown","metadata":{"id":"VWCQhb4Vtjjg"},"source":["6) You are tasked with fine-tuning an LLM to write creative content. How would you approach this?\n","\n","A) Use a pre-trained LLM without any modifications or additional training.\n","\n","B) Compile a diverse dataset of high-quality creative writing, preprocess the data, fine-tune the LLM with this dataset, and incorporate human feedback for iterative improvement.\n","\n","C) Focus solely on technical documentation and scientific articles for the dataset.\n","\n","D) Rely on the LLM's initial capabilities without any further fine-tuning or human feedback."]},{"cell_type":"markdown","metadata":{"id":"Z3EGF2qlt3eb"},"source":["7) What are the potential societal implications of widespread LLM adoption?\n","\n","A) LLMs will only affect technical fields and have no impact on non-technical industries.\n","\n","B) LLMs might reduce accessibility and productivity in content production, healthcare, and education.\n","\n","C) Widespread LLM adoption can enhance accessibility, creativity, and productivity in various fields, but it may also negatively impact jobs, propagate misinformation, and raise data privacy concerns.\n","\n","D) LLMs will eliminate the need for data privacy and ethical considerations in AI."]},{"cell_type":"markdown","metadata":{"id":"rFkbBoEsu4U4"},"source":["8) How does RAG maintain context in a conversation?\n","\n","A) RAG ignores past interactions and focuses only on the current query.\n","\n","B) RAG uses retriever component continually searching for relevant data to help the generative model produce coherent and contextually appropriate responses.\n","\n","C) RAG relies solely on predefined rules and scripts to maintain context.\n","\n","D) RAG maintains context by using a fixed database of responses without considering past interactions."]},{"cell_type":"markdown","metadata":{"id":"R3cTZJTKvULn"},"source":["9) How does RAG differ from Parameter-Efficient Fine-Tuning (PEFT)?\n","\n","A) RAG uses generative models exclusively, while PEFT focuses on retrieval-based techniques.\n","\n","B) RAG improves performance by integrating retrieval-based techniques with generative models, whereas PEFT reduces computing resources and parameters needed by optimizing pre-trained language models.\n","\n","C) RAG aims to reduce the parameters needed for fine-tuning, while PEFT focuses on improving natural language processing by using retrieval-based techniques.\n","\n","D) RAG and PEFT are both methods for reducing the size of language models without any focus on retrieval or generation."]},{"cell_type":"markdown","metadata":{"id":"8ni11IjYwFV-"},"source":["10) Can you explain how RAG models are trained?\n","\n","A) RAG models are trained only on image datasets for visual recognition tasks.\n","\n","B) RAG models are trained in a single stage focusing on real-time data retrieval.\n","\n","C) RAG models are trained in two main stages: pre-training on a large corpus of text data and fine-tuning with a retriever component to search for relevant information based on input queries.\n","\n","D) RAG models do not require any training and function based on predefined rules."]},{"cell_type":"markdown","metadata":{"id":"WXoeKW-yxp2d"},"source":["11) What is Retrieval-Augmented Generation (RAG)?\n","\n","A) A method that exclusively uses generative models for NLP tasks without any retrieval component.\n","\n","B) An approach that uses only retrieval-based methods to enhance NLP performance.\n","\n","C) A combination of retrieval-based methods and generative models, where a retriever finds relevant information, and a generative model generates a response.\n","\n","D) A technique that relies on predefined templates to generate responses in NLP tasks."]},{"cell_type":"markdown","metadata":{"id":"cK-w9Giy5zSP"},"source":["12) What are the benefits of using RAG over other NLP techniques?\n","\n","A) Increased processing speed and reduced computational requirements.\n","\n","B) Enhanced accuracy, context-awareness, flexibility, and bias and misinformation mitigation.\n","\n","C) Simplified model architecture and reduced training data requirements.\n","\n","D) Exclusive use in sentiment analysis and named entity recognition tasks."]},{"cell_type":"markdown","metadata":{"id":"Ye9P6Mji6X2S"},"source":["13) How does RAG handle bias and misinformation?\n","\n","A) By ignoring external knowledge sources and focusing solely on generative models.\n","\n","B) By using a single-step generative model approach without any retrieval component.\n","\n","C) By leveraging a two-step approach involving retrieval-based methods to prioritize credible sources and training generative models to cross-reference and validate retrieved information.\n","\n","D) By randomly selecting information from various sources without any validation mechanism."]},{"cell_type":"markdown","metadata":{"id":"JSIdoFwM6yiI"},"source":["14) How does RAG integrate with existing machine learning pipelines?\n","\n","A) By replacing all existing components with a new framework for handling numerical data.\n","\n","B) By using the retrieval component to search a database or document corpus for relevant information and the generative model to process and generate responses.\n","\n","C) By focusing solely on image processing tasks within the pipeline.\n","\n","D) By requiring a complete overhaul of the existing data sources and infrastructure."]},{"cell_type":"markdown","metadata":{"id":"Xo_yd6477ERv"},"source":["15) What is Hybrid Search of vector databases?\n","\n","A) A search technique that combines sequential search and binary search algorithms.\n","\n","B) A method that integrates full-text search and vector search to provide both keyword matches and semantic matches.\n","\n","C) A type of search that only focuses on keyword matches in documents.\n","\n","D) A process that prioritizes the speed of search over the accuracy of results."]},{"cell_type":"markdown","metadata":{"id":"YuNJBFGe74O6"},"source":["16) How does RAG differ from Parameter-Efficient Fine-Tuning (PEFT)?\n","\n","A) RAG uses data augmentation techniques, while PEFT focuses on training models from scratch.\n","\n","B) RAG combines generative models with retrieval-based techniques for producing responses, whereas PEFT reduces computing resources and parameters needed by optimizing and fine-tuning pre-trained models.\n","\n","C) RAG enhances performance through hardware improvements, while PEFT relies on increasing the size of training datasets.\n","\n","D) RAG is used for image processing tasks, whereas PEFT is exclusively for speech recognition."]},{"cell_type":"markdown","metadata":{"id":"BU4O4Xs38dzj"},"source":["17) Can you discuss the role of knowledge graphs in RAG?\n","\n","A) Knowledge graphs help RAG systems by providing visual representations of data for user interfaces.\n","\n","B) Knowledge graphs enable RAG systems to store and retrieve large volumes of unstructured data without any organization.\n","\n","C) Knowledge graphs enhance RAG systems by offering structured representations of knowledge and relationships, improving information retrieval and reasoning.\n","\n","D) Knowledge graphs are primarily used in RAG systems for generating random user responses without context."]},{"cell_type":"markdown","metadata":{"id":"BoyyGM918seI"},"source":["18) What are the limitations of RAG?\n","\n","A) Lack of support for natural language processing tasks.\n","\n","B) Inability to generate any responses without manual intervention.\n","\n","C) Computational complexity, dependency on data quality, scalability issues, and the risk of propagating bias and misinformation.\n","\n","D) Limited to processing only small datasets and short documents.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vwZJT9AU9cgu"},"source":["19) How does RAG maintain context in a conversation?\n","\n","A) By using a predefined set of responses regardless of the current conversation.\n","\n","B) By relying solely on the initial input without considering past interactions.\n","\n","C) By constantly searching for and retrieving pertinent data from past encounters or the current conversation, enabling the generative model to produce coherent and contextually appropriate replies.\n","\n","D) By ignoring the existing conversation and generating random responses."]},{"cell_type":"markdown","metadata":{"id":"C_l3taTs9gLK"},"source":["20) In what ways can RAG enhance human-AI collaboration?\n","\n","A) By limiting access to large datasets and document corpora for precise retrieval.\n","\n","B) By focusing on isolated context to generate more straightforward replies.\n","\n","C) By improving retrieval of information from big datasets and document corpora, maintaining context consistency, and offering customized responses based on user preferences.\n","\n","D) By reducing the reliance on external knowledge sources and focusing solely on past interactions."]},{"cell_type":"markdown","metadata":{"id":"_fOH13FMqgrF"},"source":["# 2. Thực Hành"]},{"cell_type":"markdown","metadata":{"id":"Eku20oRBagGa"},"source":["Học viên lựa chọn một trong số các bài tập sau đây để thực hiện phần thực hành.\n","\n","Lưu ý: Bài thực hành cần giải thích rõ ràng các phần:\n","- Xử lý dữ liệu\n","- Khởi tạo mô hình\n","- Huấn luyện mô hình\n","- Đánh giá mô hình\n","- Inference mô hình\n","- Visualize các metric và loss function"]},{"cell_type":"markdown","metadata":{"id":"pyoJIV6ZXgX9"},"source":["1) Hãy xây dựng một mô hình cho bài toán phân loại tin tức từ đầu vào là bộ dữ liệu [Amazon Reviews](https://www.kaggle.com/datasets/bittlingmayer/amazonreviews)."]},{"cell_type":"markdown","metadata":{"id":"L_t8z29eYEMp"},"source":["2) Hãy xây dựng mô hình cho bài toán nhận diện thực thể (Named Entity Recognition) từ dầu vào là bộ dữ liệu [Entity Anotanted Corpus](https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus)\n","\n","Gợi ý: Xem hướng dẫn https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jiqcV3l0ZEyD"},"source":["3) Hãy xây dựng một mô hình chatbot từ đầu vào là bộ dữ liệu [Alpaca-Lora](https://raw.githubusercontent.com/tloen/alpaca-lora/main/alpaca_data.json)\n","\n","Gợi ý: Có thể tham khảo một trong các nguồn sau đây để finetuning chatbot.\n","- https://github.com/tloen/alpaca-lora\n","- https://colab.research.google.com/drive/1GjtZ7pFbg8NoRIQE0f3TqE4nU5KEU3lW?usp=drive_link"]},{"cell_type":"markdown","metadata":{"id":"7xRPgMXNw1JR"},"source":["4) Finetuning một mô hình LLM cho bài toán phân loại tin tức [Twitter Sentiment Analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","dataPath = './Data/test.ft.txt'\n","\n","def parse_data(file_path):\n","    data = []\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            label, text = line.split(' ', 1)\n","            label = int(label.replace('__label__', ''))\n","            data.append((label, text.strip()))\n","    return pd.DataFrame(data, columns=['label', 'text'])"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>Great CD: My lovely Pat has one of the GREAT v...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>One of the best game music soundtracks - for a...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>Batteries died within a year ...: I bought thi...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>works fine, but Maha Energy is better: Check o...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>Great for the non-audiophile: Reviewed quite a...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>399995</th>\n","      <td>1</td>\n","      <td>Unbelievable- In a Bad Way: We bought this Tho...</td>\n","    </tr>\n","    <tr>\n","      <th>399996</th>\n","      <td>1</td>\n","      <td>Almost Great, Until it Broke...: My son reciev...</td>\n","    </tr>\n","    <tr>\n","      <th>399997</th>\n","      <td>1</td>\n","      <td>Disappointed !!!: I bought this toy for my son...</td>\n","    </tr>\n","    <tr>\n","      <th>399998</th>\n","      <td>2</td>\n","      <td>Classic Jessica Mitford: This is a compilation...</td>\n","    </tr>\n","    <tr>\n","      <th>399999</th>\n","      <td>1</td>\n","      <td>Comedy Scene, and Not Heard: This DVD will be ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>400000 rows × 2 columns</p>\n","</div>"],"text/plain":["        label                                               text\n","0           2  Great CD: My lovely Pat has one of the GREAT v...\n","1           2  One of the best game music soundtracks - for a...\n","2           1  Batteries died within a year ...: I bought thi...\n","3           2  works fine, but Maha Energy is better: Check o...\n","4           2  Great for the non-audiophile: Reviewed quite a...\n","...       ...                                                ...\n","399995      1  Unbelievable- In a Bad Way: We bought this Tho...\n","399996      1  Almost Great, Until it Broke...: My son reciev...\n","399997      1  Disappointed !!!: I bought this toy for my son...\n","399998      2  Classic Jessica Mitford: This is a compilation...\n","399999      1  Comedy Scene, and Not Heard: This DVD will be ...\n","\n","[400000 rows x 2 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["train = parse_data(dataPath)\n","\n","dataset_amazon_review"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n"]},{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Collecting transformers\n","  Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n","     ---------------------------------------- 9.9/9.9 MB 4.3 MB/s eta 0:00:00\n","Collecting seqeval[gpu]\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","     -------------------------------------- 43.6/43.6 kB 354.9 kB/s eta 0:00:00\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'done'\n","Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n","Collecting safetensors>=0.4.1\n","  Downloading safetensors-0.4.5-cp39-none-win_amd64.whl (286 kB)\n","     -------------------------------------- 286.1/286.1 kB 4.4 MB/s eta 0:00:00\n","Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n","Collecting tokenizers<0.21,>=0.20\n","  Downloading tokenizers-0.20.1-cp39-none-win_amd64.whl (2.4 MB)\n","     ---------------------------------------- 2.4/2.4 MB 3.8 MB/s eta 0:00:00\n","Collecting huggingface-hub<1.0,>=0.23.2\n","  Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n","     -------------------------------------- 436.6/436.6 kB 3.4 MB/s eta 0:00:00\n","Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n","Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from seqeval[gpu]) (1.0.2)\n","Collecting fsspec>=2023.5.0\n","  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","     -------------------------------------- 179.3/179.3 kB 2.7 MB/s eta 0:00:00\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lehoa\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.9.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (2.2.0)\n","Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n","Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py): started\n","  Building wheel for seqeval (setup.py): finished with status 'done'\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=0e564d39ba912c6297ea4a1dddaf865520c0f9268a647c6f76f9e52444005a63\n","  Stored in directory: c:\\users\\lehoa\\appdata\\local\\pip\\cache\\wheels\\e2\\a5\\92\\2c80d1928733611c2747a9820e1324a6835524d9411510c142\n","Successfully built seqeval\n","Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, seqeval, transformers\n","Successfully installed fsspec-2024.9.0 huggingface-hub-0.25.2 safetensors-0.4.5 seqeval-1.2.2 tokenizers-0.20.1 transformers-4.45.2\n"]}],"source":["!pip install transformers seqeval[gpu]"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO4G4J8eEto3LPtMBTINZy/","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
