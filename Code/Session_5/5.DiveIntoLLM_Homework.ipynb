{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"N8lRbrs06TnV"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (691898065.py, line 1)","output_type":"error","traceback":["\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\lehoa\\AppData\\Local\\Temp\\ipykernel_1760\\691898065.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    1. C\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"]}],"source":["# 1. C\n","# 2. B\n","# 3. C\n","# 4. B\n","# 5. B\n","# 6. C\n","# 7. A\n","# 8. C\n","# 9. C\n","# 10. C"]},{"cell_type":"markdown","metadata":{"id":"1jMmiMCMTTrt"},"source":["# 1. Lý thuyết (10 câu, 0.5 điểm/câu)"]},{"cell_type":"markdown","metadata":{"id":"iJI2kLGxTXTg"},"source":["1) what is a Large Language Model (LLM)?\n","\n","A) A computer program designed to play complex strategy games at a human level.\n","\n","B) A type of database system optimized for storing and retrieving large volumes of text data.\n","\n","C) An artificial intelligence system educated on copious volumes of textual material to comprehend and produce language like humans.\n","\n","D) A software application used for managing and automating business processes."]},{"cell_type":"markdown","metadata":{"id":"YJDSdfkhTY5M"},"source":["2) What differentiates LLMs from traditional chatbots?\n","\n","A) Traditional chatbots are designed to play games, while LLMs are not.\n","\n","B) Traditional chatbots are constrained by preset guidelines and rule-based frameworks, whereas LLMs are trained on vast quantities of data, allowing them to comprehend and produce language more naturally.\n","\n","C) Traditional chatbots can handle more complex and open-ended conversations than LLMs.\n","\n","D) LLMs are constrained by a predetermined list of answers, unlike traditional chatbots."]},{"cell_type":"markdown","metadata":{"id":"xvcZKmehT9yY"},"source":["3) Describe some techniques for evaluating the performance of LLMs.\n","\n","A) Evaluating their ability to play chess and solve math problems.\n","\n","B) Measuring ambiguity and using the RGB score for text color accuracy.\n","\n","C) Using perplexity to assess prediction accuracy, BLEU score for translation quality, and human raters for coherence, relevance, and factual accuracy.\n","\n","D) Checking their speed and efficiency in processing large datasets."]},{"cell_type":"markdown","metadata":{"id":"5hFn7xJBUbMj"},"source":["4) Can prompt engineering be used to improve LLM outputs?\n","\n","A) No, prompt engineering has no impact on LLM outputs.\n","\n","B) Yes, by carefully constructing input prompts, developers can guide LLM outputs to be more pertinent, logical, and aligned with certain objectives, improving factual accuracy and reducing biases.\n","\n","C) Yes, but only for generating visual content, not for text-based tasks.\n","\n","D) No, LLM outputs are entirely random and cannot be influenced by input prompts."]},{"cell_type":"markdown","metadata":{"id":"2OoOFFVrVRH9"},"source":["5) What are some ethical considerations surrounding the use of LLMs?\n","\n","A) Ensuring LLMs can generate content in multiple languages.\n","\n","B) Addressing concerns related to privacy and data protection, bias and discrimination, intellectual property, misuse and malicious applications, and environmental impact.\n","\n","C) Improving the speed and efficiency of LLM training processes.\n","\n","D) Increasing the diversity of training datasets to include more non-English text."]},{"cell_type":"markdown","metadata":{"id":"CikqBCfGVio7"},"source":["6) Explain the concept of few-shot learning and its applications in fine-tuning LLMs.\n","\n","A) Few-shot learning involves using a large number of labeled instances to train LLMs for specific tasks or domains.\n","\n","B) Few-shot learning requires zero labeled instances and relies entirely on unsupervised learning techniques.\n","\n","C) Few-shot learning is a strategy where LLMs are fine-tuned with a limited number of labeled instances (usually 1 to 5) to quickly learn and generalize for specific tasks or domains, particularly useful when large labeled datasets are not available.\n","\n","D) Few-shot learning is exclusively used for enhancing the speed of LLMs without impacting their accuracy or generalization capabilities."]},{"cell_type":"markdown","metadata":{"id":"FIRV3kTOVpcU"},"source":["7) What are the challenges associated with large-scale deployment of LLMs in real-world applications?\n","\n","A) Insufficient computing resources and high energy consumption, data privacy concerns, maintaining model accuracy over time, addressing biases, integrating into existing systems, and ensuring legal compliance and ethical standards.\n","\n","B) Limited availability of training data, slow inference speeds, and high costs associated with model deployment.\n","\n","C) Challenges related to hardware compatibility, software updates, and user interface design for LLMs.\n","\n","D) Difficulties in understanding complex algorithms and their application in real-time scenarios."]},{"cell_type":"markdown","metadata":{"id":"NzdPQLvGWHcH"},"source":["8) How can the explainability and interpretability of LLM decisions be improved?\n","\n","A) By decreasing the complexity of LLM architectures and limiting the use of attention mechanisms.\n","\n","B) Through the integration of human-in-the-loop techniques and ignoring internal model representations.\n","\n","C) By incorporating interpretable modules, analyzing internal representations, employing counterfactual explanations, and utilizing human-in-the-loop approaches.\n","\n","D) By increasing the size of training datasets and focusing solely on external validation metrics."]},{"cell_type":"markdown","metadata":{"id":"lAlgFhUQWlLv"},"source":["9) Explain the concept of self-attention and its role in LLM performance.\n","\n","A) Self-attention enhances LLM performance by focusing only on the beginning and end of input sequences.\n","\n","B) Self-attention allows LLMs to ignore contextual information and focus solely on word frequencies.\n","\n","C) Self-attention in transformer architecture enables LLMs to assign varying weights to different parts of input sequences, capturing contextual information and long-range dependencies effectively.\n","\n","D) Self-attention increases the model's reliance on sequential processing, which improves performance in structured data tasks."]},{"cell_type":"markdown","metadata":{"id":"GBHnZ4DQW_yz"},"source":["10) What resources would you use to stay updated on the latest advancements in LLMs?\n","\n","A) Following only academic publications like NeurIPS and ACL for updates on LLMs.\n","\n","B) Keeping track of commercial resources such as tech magazines and blogs.\n","\n","C) Utilizing a combination of academic publications like NeurIPS and commercial resources like tech firm blogs and social media.\n","\n","D) Attending webinars exclusively for firsthand insights into LLM advancements."]},{"cell_type":"markdown","metadata":{"id":"EB4MI4RHTVxe"},"source":["# 2. Thực hành"]},{"cell_type":"markdown","metadata":{"id":"Eku20oRBagGa"},"source":["Học viên lựa chọn một trong số các bài tập sau đây để thực hiện phần thực hành.\n","\n","Lưu ý: Bài thực hành cần giải thích rõ ràng các phần:\n","- Xử lý dữ liệu\n","- Khởi tạo mô hình\n","- Huấn luyện mô hình\n","- Đánh giá mô hình\n","- Inference mô hình\n","- Visualize các metric và loss function"]},{"cell_type":"markdown","metadata":{"id":"pyoJIV6ZXgX9"},"source":["1) Hãy xây dựng một mô hình cho bài toán phân loại tin tức từ đầu vào là bộ dữ liệu [Amazon Reviews](https://www.kaggle.com/datasets/bittlingmayer/amazonreviews)."]},{"cell_type":"markdown","metadata":{"id":"L_t8z29eYEMp"},"source":["2) Hãy xây dựng mô hình cho bài toán nhận diện thực thể (Named Entity Recognition) từ dầu vào là bộ dữ liệu [Entity Anotanted Corpus](https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus)\n","\n","Gợi ý: Xem hướng dẫn https://huggingface.co/learn/nlp-course/chapter7/2?fw=pt\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jiqcV3l0ZEyD"},"source":["3) Hãy xây dựng một mô hình chatbot từ đầu vào là bộ dữ liệu [Alpaca-Lora](https://raw.githubusercontent.com/tloen/alpaca-lora/main/alpaca_data.json)\n","\n","Gợi ý: Có thể tham khảo một trong các nguồn sau đây để finetuning chatbot.\n","- https://github.com/tloen/alpaca-lora\n","- https://drive.google.com/file/d/1QfTZM6Mr6R0CAKipEQjjqiZguVymwTYT/view?usp=drive_link"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPvZPft3u7BDBA4dfn0glDW","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
